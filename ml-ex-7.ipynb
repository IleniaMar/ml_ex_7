{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c5d08680","cell_type":"markdown","source":"# **Reti Neurali** ","metadata":{}},{"id":"52a5f30f","cell_type":"markdown","source":"## **Esercizio 1: Download e pre-processamento dei dati.**\n\nScaricare e pre-processare i dati per il successivo addestramento del modello. \n\nIl dataset che utilizzeremo sarà CIFAR10, scaricabile dalla libreria `tensorflow.keras.datasets`","metadata":{}},{"id":"42e9791d","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\n\n# Download dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\n# Stampare le shape dei dati\nprint(f\"x_Training set: {x_train.shape}\")\nprint(f\"x_Test set: {x_test.shape}\")\nprint(f\"y_Training set: {y_train.shape}\")\nprint(f\"y_Test set: {y_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:38:03.536369Z","iopub.execute_input":"2025-05-27T13:38:03.536753Z","iopub.status.idle":"2025-05-27T13:38:30.551293Z","shell.execute_reply.started":"2025-05-27T13:38:03.536720Z","shell.execute_reply":"2025-05-27T13:38:30.550358Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 13:38:05.489374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748353085.754849      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748353085.827457      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\nx_Training set: (50000, 32, 32, 3)\nx_Test set: (10000, 32, 32, 3)\ny_Training set: (50000,)\ny_Test set: (10000,)\n","output_type":"stream"}],"execution_count":1},{"id":"4d6a0ca6","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Pre-processamento dei dati\n\n# svolgimento...\n\nx_train = x_train.reshape(x_train.shape[0], -1)  # Flatten the images\nx_test = x_test.reshape(x_test.shape[0], -1)  # Flatten the images\nscaler = StandardScaler()\nx_train= scaler.fit_transform(x_train)\nx_test= scaler.transform(x_test)\n\nprint(f\"x_Training_scaler: {x_train.shape}\")\nprint(f\"x_Test_scaler: {x_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:38:37.247925Z","iopub.execute_input":"2025-05-27T13:38:37.248242Z","iopub.status.idle":"2025-05-27T13:38:42.224835Z","shell.execute_reply.started":"2025-05-27T13:38:37.248221Z","shell.execute_reply":"2025-05-27T13:38:42.223816Z"}},"outputs":[{"name":"stdout","text":"x_Training_scaler: (50000, 3072)\nx_Test_scaler: (10000, 3072)\n","output_type":"stream"}],"execution_count":2},{"id":"5103ad7f","cell_type":"markdown","source":"## **Esercizio 2: Creare modello MLP**\n\nPer creare il modello MLP utilizziamo l' oggetto `MLPClassifier` dal modulo `sklearn.neural_networks`. Questo è un oggetto molto complesso che prevede la possibilità di specificare tanti parametri, permettendoci una personalizzazione molto dettagliata. Vediamo di seguito gli argomenti principali:\n\n- `hidden_layer_sizes`: rappresenta la struttura dell' MLP, sotto forma di una tupla. La tupla deve essere composta da numeri interi, ogni numero indica il numero di neuroni presenti nel rispettivo layer.\n\nEsempio: \n\n`hidden_layer_sizes` = `(100)`\n\ncreerà un solo layer con 100 neuroni\n\n`hidden_layer_sizes` = `(100, 50)`\n\ncreerà due layer, il primo con 100 neuroni, il secondo invece con 50.\n\n- `max_iter`: massimo numero di iterazioni per raggiungere la convergenza. \n\n- `activation`: indica quale funzione di attivazione utilizzare, valori possibili sono `'relu'`, `'logistic'`, `'tanh'` and `'identity'`.\n\n- `solver`: indica quale algoritmo di ottimizzazione utilizzare, valori possibili sono `'adam'`, `'sgd'` and `'lbfgs'`.\n\n- `learning_rate_init`: valore iniziale del learning rate.\n\n- `verbose`: valore booleano che, se impostato su `True`, stampa l' output di ogni iterazione di training. Molto utile per monitorare il training.\n\n- `random_state`: fissa il seed della randomizzazione.\n","metadata":{}},{"id":"b0f857d0","cell_type":"markdown","source":"Per iniziare creiamo un MLP molto basilare, alleniamolo e testiamone le performance. Come parametri utilizzeremo:\n\n- `hidden_layer_sizes` = `(100)`\n\n- `max_iter` = `20`\n\n- `random_state` = `42`","metadata":{}},{"id":"4650894a","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP \n\n# svolgimento...\nMLP = MLPClassifier(hidden_layer_sizes = (100), max_iter = 20, random_state = 42)\n\n# Allenare MLP\n\n# svolgimento...\nMLP.fit(x_train, y_train)\n\n\n# Valutare MLP\n\n# svolgimento...\nprediction = MLP.predict(x_test)\n\n\n# Stampare l' accuratezza\n\n# svolgimento...\naccuracy_test = accuracy_score(y_test, prediction)\nprint(f\"Accuracy test {accuracy_test}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:38:46.443466Z","iopub.execute_input":"2025-05-27T13:38:46.443804Z","iopub.status.idle":"2025-05-27T13:39:55.704339Z","shell.execute_reply.started":"2025-05-27T13:38:46.443781Z","shell.execute_reply":"2025-05-27T13:39:55.703280Z"}},"outputs":[{"name":"stdout","text":"Accuracy test 0.5057\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"id":"6171280e","cell_type":"markdown","source":"## **Esercizio 2.1: Aumentiamo i parametri del nostro modello**\n\nProviamo adesso ad aumentare i dettagli del nostro modello, modificando o aggiungendo i parametri sopra specificati. ","metadata":{}},{"id":"f35afa68","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP con più strati e altre specifiche\n\n# svolgimento...\n\nMLP = MLPClassifier(hidden_layer_sizes = (100,50), max_iter = 100, random_state = 42, verbose = True, activation = 'logistic', solver = 'sgd')\n\nMLP.fit(x_train, y_train)\n\nprediction = MLP.predict(x_test)\n\naccuracy_test = accuracy_score(y_test, prediction)\nprint(f\"Accuracy test {accuracy_test}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:40:06.871537Z","iopub.execute_input":"2025-05-27T13:40:06.871878Z","iopub.status.idle":"2025-05-27T13:45:13.852371Z","shell.execute_reply.started":"2025-05-27T13:40:06.871853Z","shell.execute_reply":"2025-05-27T13:45:13.851423Z"}},"outputs":[{"name":"stdout","text":"Iteration 1, loss = 2.29411142\nIteration 2, loss = 2.25892467\nIteration 3, loss = 2.23094289\nIteration 4, loss = 2.19986364\nIteration 5, loss = 2.16612044\nIteration 6, loss = 2.13210478\nIteration 7, loss = 2.10018741\nIteration 8, loss = 2.07131360\nIteration 9, loss = 2.04606504\nIteration 10, loss = 2.02407949\nIteration 11, loss = 2.00496407\nIteration 12, loss = 1.98816671\nIteration 13, loss = 1.97316024\nIteration 14, loss = 1.95958031\nIteration 15, loss = 1.94734572\nIteration 16, loss = 1.93607822\nIteration 17, loss = 1.92569921\nIteration 18, loss = 1.91603717\nIteration 19, loss = 1.90735138\nIteration 20, loss = 1.89893509\nIteration 21, loss = 1.89115769\nIteration 22, loss = 1.88351279\nIteration 23, loss = 1.87622130\nIteration 24, loss = 1.86914419\nIteration 25, loss = 1.86214122\nIteration 26, loss = 1.85527961\nIteration 27, loss = 1.84859691\nIteration 28, loss = 1.84205296\nIteration 29, loss = 1.83552843\nIteration 30, loss = 1.82920156\nIteration 31, loss = 1.82298501\nIteration 32, loss = 1.81684709\nIteration 33, loss = 1.81091774\nIteration 34, loss = 1.80522616\nIteration 35, loss = 1.79958778\nIteration 36, loss = 1.79417563\nIteration 37, loss = 1.78893908\nIteration 38, loss = 1.78388129\nIteration 39, loss = 1.77898231\nIteration 40, loss = 1.77426186\nIteration 41, loss = 1.76968682\nIteration 42, loss = 1.76512665\nIteration 43, loss = 1.76085870\nIteration 44, loss = 1.75669778\nIteration 45, loss = 1.75261341\nIteration 46, loss = 1.74854899\nIteration 47, loss = 1.74466951\nIteration 48, loss = 1.74083227\nIteration 49, loss = 1.73690227\nIteration 50, loss = 1.73318811\nIteration 51, loss = 1.72962877\nIteration 52, loss = 1.72588963\nIteration 53, loss = 1.72233863\nIteration 54, loss = 1.71865750\nIteration 55, loss = 1.71520080\nIteration 56, loss = 1.71166766\nIteration 57, loss = 1.70817949\nIteration 58, loss = 1.70476539\nIteration 59, loss = 1.70133129\nIteration 60, loss = 1.69794582\nIteration 61, loss = 1.69463386\nIteration 62, loss = 1.69118747\nIteration 63, loss = 1.68793410\nIteration 64, loss = 1.68466738\nIteration 65, loss = 1.68140050\nIteration 66, loss = 1.67828370\nIteration 67, loss = 1.67499856\nIteration 68, loss = 1.67186263\nIteration 69, loss = 1.66878814\nIteration 70, loss = 1.66560251\nIteration 71, loss = 1.66254427\nIteration 72, loss = 1.65966896\nIteration 73, loss = 1.65677159\nIteration 74, loss = 1.65370332\nIteration 75, loss = 1.65090961\nIteration 76, loss = 1.64795629\nIteration 77, loss = 1.64521872\nIteration 78, loss = 1.64233681\nIteration 79, loss = 1.63952057\nIteration 80, loss = 1.63691069\nIteration 81, loss = 1.63422903\nIteration 82, loss = 1.63136718\nIteration 83, loss = 1.62868384\nIteration 84, loss = 1.62612863\nIteration 85, loss = 1.62344077\nIteration 86, loss = 1.62101503\nIteration 87, loss = 1.61841875\nIteration 88, loss = 1.61589686\nIteration 89, loss = 1.61334497\nIteration 90, loss = 1.61085612\nIteration 91, loss = 1.60837039\nIteration 92, loss = 1.60583819\nIteration 93, loss = 1.60345874\nIteration 94, loss = 1.60106260\nIteration 95, loss = 1.59866293\nIteration 96, loss = 1.59637072\nIteration 97, loss = 1.59381634\nIteration 98, loss = 1.59143184\nIteration 99, loss = 1.58927971\nIteration 100, loss = 1.58692012\nAccuracy test 0.4336\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"id":"226b1681","cell_type":"markdown","source":"## **Esercizio 3: Implementare manualmente l' algoritmo di early stopping.**\n\nL' algoritmo di early stopping ci permette di terminare anticipatamente l' allenamento di un modello nel caso in cui questo raggiunga la convergenza. Supponiamo infatti che il nostro modello raggiunga un certo livello di accuratezza e che non riesca a migliorare oltre quel livello. Questo significa che il modello, da quel momento in poi, non sta più apprendendo nuove informazioni, per cui le successive iterazioni sono superflue, ed inoltre rischiano di essere dannose, spingendo il modello verso l' overfitting. \n\nL' early stopping verifica ad ogni iterazione che l' accuratezza del modello sia incrementata di una certa tolleranza. Se questa tolleranza non viene superata per un certo numero di epoche, allora possiamo decidere di stoppare l' allenamento in quanto il modello ha raggiunto la convergenza.\n\n**N.B: per applicare early stopping è necessario specificare i seguenti parametri dell' MLP:**\n\n- `warm_start`=`True` in modo che il training proceda dallo stato attuale del modello e non dall' inizializzazione.\n\n- `max_iter`=`1` in modo che il modello venga allenato per una sola epoca. Per l' early stopping infatti dovremo gestire manualmente il numero di iterazioni.","metadata":{}},{"id":"06947f7f-4bbf-4477-93e9-c76332ceb202","cell_type":"code","source":"train_fraction = 0.8  \nval_fraction = 0.2\n\nnum_train = int(train_fraction * x_train.shape[0])\nx_train_1 = x_train[:num_train]\ny_train_1 = y_train[:num_train]\n\nx_val = x_train[num_train:]\ny_val = y_train[num_train:]\n\nprint(f\"Training set: {x_train_1.shape}, {y_train_1.shape}\")\nprint(f\"Validation set: {x_val.shape}, {y_val.shape}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:45:34.401400Z","iopub.execute_input":"2025-05-27T13:45:34.402610Z","iopub.status.idle":"2025-05-27T13:45:34.409022Z","shell.execute_reply.started":"2025-05-27T13:45:34.402516Z","shell.execute_reply":"2025-05-27T13:45:34.407711Z"}},"outputs":[{"name":"stdout","text":"Training set: (40000, 3072), (40000,)\nValidation set: (10000, 3072), (10000,)\n","output_type":"stream"}],"execution_count":5},{"id":"8ee1709d","cell_type":"code","source":"import numpy as np\nfrom sklearn.neural_network import MLPClassifier \nfrom sklearn.metrics import accuracy_score \n\n\nn_total_epochs = 100  \npatience = 10         \ntolerance = 1e-4      \n\nbest_val_accuracy = 0.0\nepochs_without_improvement = 0\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:45:38.011503Z","iopub.execute_input":"2025-05-27T13:45:38.011821Z","iopub.status.idle":"2025-05-27T13:45:38.017761Z","shell.execute_reply.started":"2025-05-27T13:45:38.011800Z","shell.execute_reply":"2025-05-27T13:45:38.016625Z"}},"outputs":[],"execution_count":6},{"id":"9ddace1e","cell_type":"code","source":"MLP = MLPClassifier(hidden_layer_sizes = (100,50), max_iter = 1, random_state = 42, verbose = True, warm_start=True)\n\nfor e in range(n_total_epochs):\n   \n    MLP.fit(x_train_1, y_train_1)\n\n   \n    val_preds = MLP.predict(x_val)\n    \n    val_accuracy = accuracy_score(y_val, val_preds)\n    print(f\"Epoch {e+1}: Validation Accuracy = {val_accuracy:.4f}\")\n\n    if val_accuracy > best_val_accuracy + tolerance:\n        best_val_accuracy = val_accuracy\n        epochs_without_improvement = 0\n        \n        best_weights = (MLP.coefs_.copy(), MLP.intercepts_.copy())\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= patience:\n        print(f\"Early stopping at epoch {e+1}\")\n        break\n\nMLP.coefs_, MLP.intercepts_ = best_weights\n\n# Valutazione finale sul test set\ntest_preds = MLP.predict(x_test)\ntest_accuracy = accuracy_score(y_test, test_preds)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:45:41.539542Z","iopub.execute_input":"2025-05-27T13:45:41.539933Z","iopub.status.idle":"2025-05-27T13:46:35.098287Z","shell.execute_reply.started":"2025-05-27T13:45:41.539904Z","shell.execute_reply":"2025-05-27T13:46:35.097175Z"}},"outputs":[{"name":"stdout","text":"Iteration 1, loss = 1.79120314\nEpoch 1: Validation Accuracy = 0.4142\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2, loss = 1.58212764\nEpoch 2: Validation Accuracy = 0.4381\nIteration 3, loss = 1.46620522\nEpoch 3: Validation Accuracy = 0.4542\nIteration 4, loss = 1.38285380\nEpoch 4: Validation Accuracy = 0.4557\nIteration 5, loss = 1.31646689\nEpoch 5: Validation Accuracy = 0.4659\nIteration 6, loss = 1.26045658\nEpoch 6: Validation Accuracy = 0.4693\nIteration 7, loss = 1.21279117\nEpoch 7: Validation Accuracy = 0.4786\nIteration 8, loss = 1.17302869\nEpoch 8: Validation Accuracy = 0.4697\nIteration 9, loss = 1.13636410\nEpoch 9: Validation Accuracy = 0.4712\nIteration 10, loss = 1.10059932\nEpoch 10: Validation Accuracy = 0.4699\nIteration 11, loss = 1.06994370\nEpoch 11: Validation Accuracy = 0.4715\nIteration 12, loss = 1.04334986\nEpoch 12: Validation Accuracy = 0.4725\nIteration 13, loss = 1.01476875\nEpoch 13: Validation Accuracy = 0.4727\nIteration 14, loss = 0.98730309\nEpoch 14: Validation Accuracy = 0.4675\nIteration 15, loss = 0.96576066\nEpoch 15: Validation Accuracy = 0.4658\nIteration 16, loss = 0.94760061\nEpoch 16: Validation Accuracy = 0.4684\nIteration 17, loss = 0.92232228\nEpoch 17: Validation Accuracy = 0.4625\nEarly stopping at epoch 17\nTest Accuracy: 0.4638\n","output_type":"stream"}],"execution_count":7}]}